import os
import glob
import streamlit as st
import sys
import pysqlite3 # <<< Bu satÄ±r en Ã¼ste taÅŸÄ±ndÄ±

# pysqlite3'Ã¼ sistemin varsayÄ±lan sqlite3'Ã¼ olarak ayarla
sys.modules["sqlite3"] = sys.modules["pysqlite3"]

# Yeni ve gÃ¼ncellenmiÅŸ import'lar
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from chromadb.config import Settings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationSummaryMemory
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.prompts import PromptTemplate
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

# --- RAG Sisteminin HazÄ±rlanmasÄ± ---
@st.cache_resource
def setup_rag_system():
    db_path = "./chroma_db"
    files_dir = "./files"

    if not os.path.exists(files_dir) or not glob.glob(os.path.join(files_dir, "*.pdf")):
        st.error(f"'{files_dir}' klasÃ¶rÃ¼ veya iÃ§inde PDF dosyalarÄ± bulunamadÄ±. LÃ¼tfen bu klasÃ¶rÃ¼ ve dosyalarÄ± ekleyin.")
        return None

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    if os.path.exists(db_path) and os.path.isdir(db_path):
        try:
            st.info("Mevcut veritabanÄ± bulunuyor. YÃ¼kleniyor...")
            vectorstore = Chroma(
                collection_name="parent_child_collection",
                embedding_function=embeddings,
                persist_directory=db_path
            )
            store = InMemoryStore() 
            retriever = ParentDocumentRetriever(
                vectorstore=vectorstore,
                docstore=store,
                parent_splitter=RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200),
                child_splitter=RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50),
            )
            st.success("VeritabanÄ± baÅŸarÄ±yla yÃ¼klendi.")
            return retriever
        except Exception as e:
            st.warning(f"VeritabanÄ± yÃ¼klenirken bir hata oluÅŸtu: {e}. Yeniden oluÅŸturuluyor...")
            
    st.info("VeritabanÄ± bulunamadÄ±. Yeni bir veritabanÄ± oluÅŸturuluyor...")
    pdf_files = glob.glob(os.path.join(files_dir, "*.pdf"))
    all_documents = []
    for file_path in pdf_files:
        st.info(f"'{os.path.basename(file_path)}' dosyasÄ± yÃ¼kleniyor...")
        loader = PyPDFLoader(file_path)
        all_documents.extend(loader.load())

    st.success(f"Toplam {len(all_documents)} sayfa yÃ¼klendi.")
    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)

    vectorstore = Chroma.from_documents(
        documents=all_documents,
        embedding=embeddings,
        collection_name="parent_child_collection",
        persist_directory=db_path
    )
    store = InMemoryStore()

    retriever = ParentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=store,
        parent_splitter=parent_splitter,
        child_splitter=child_splitter,
    )
    st.success("GeliÅŸmiÅŸ indeksleme ve bellek tabanlÄ± geri alma sistemi hazÄ±r.")
    return retriever

# --- Streamlit ArayÃ¼zÃ¼ ---
st.set_page_config(page_title="YarÄ±ÅŸma AsistanÄ±", layout="wide")
st.title("ðŸ† YarÄ±ÅŸma AsistanÄ±")
st.write("Åžartnameler ve raporlar hakkÄ±nda sorularÄ±nÄ±zÄ± sorun.")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "llm" not in st.session_state:
    st.session_state.llm = None
if "memory" not in st.session_state:
    st.session_state.memory = None

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

retriever = setup_rag_system()
if retriever:
    if st.session_state.qa_chain is None:
        st.session_state.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            temperature=0.7,
            google_api_key=os.environ.get("GOOGLE_API_KEY")
        )
        st.session_state.memory = ConversationSummaryMemory(
            llm=st.session_state.llm,
            memory_key="chat_history", 
            return_messages=True
        )
        
        custom_prompt_template = """
Sen 2204-A yarÄ±ÅŸmasÄ±na hazÄ±rlanan Ã¶ÄŸrenci ve danÄ±ÅŸman Ã¶ÄŸretmenlere yardÄ±mcÄ± olan bir asistansÄ±n. Ã–ÄŸrenci ve Ã¶ÄŸretmenlere neyi nasÄ±l yapmalarÄ± gerektiÄŸi konusunda rehberlik ediyorsun.
AÅŸaÄŸÄ±daki konuÅŸma geÃ§miÅŸini ve baÄŸlamÄ± kullanarak, en son kullanÄ±cÄ± sorusuna kÄ±sa ve net bir yanÄ±t ver.
CevabÄ±nÄ± doÄŸrudan baÄŸlamdaki bilgilerden al. EÄŸer baÄŸlamda sorunun cevabÄ± yoksa, "Bu konuda ÅŸartnamede net bir bilgi bulunmamaktadÄ±r." ÅŸeklinde yanÄ±t ver.
Kesinlikle baÄŸlamda olmayan bir bilgi uydurma.

KonuÅŸma GeÃ§miÅŸi:
{chat_history}

BaÄŸlam:
{context}

Soru:
{question}

YardÄ±mcÄ± AsistanÄ±n CevabÄ±:
"""
        CUSTOM_PROMPT = PromptTemplate(
            template=custom_prompt_template,
            input_variables=["chat_history", "context", "question"]
        )
        
        multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=retriever,
            llm=st.session_state.llm
        )
        
        st.session_state.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=st.session_state.llm,
            retriever=multi_query_retriever,
            memory=st.session_state.memory,
            rephrase_question=False,
            combine_docs_chain_kwargs={"prompt": CUSTOM_PROMPT} 
        )

    if prompt := st.chat_input("Buraya yazÄ±n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("Cevap bekleniyor..."):
            retrieved_docs = retriever.invoke(prompt)
            total_retrieved_length = sum(len(doc.page_content) for doc in retrieved_docs)

            if total_retrieved_length < 100:
                general_llm_response = st.session_state.llm.invoke(prompt)
                response = general_llm_response.content
                st.session_state.memory.save_context({"input": prompt}, {"output": response})
            else:
                result = st.session_state.qa_chain.invoke({"question": prompt})
                response = result["answer"]
            
        with st.chat_message("assistant"):
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
else:
    st.error("Proje baÅŸlatÄ±lamÄ±yor. LÃ¼tfen gerekli dosyalarÄ±n ve Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.")
